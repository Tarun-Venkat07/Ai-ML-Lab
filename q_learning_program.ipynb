{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWIA34sBAnhU",
        "outputId": "99e09fb6-e115-4cf9-ab07-94186d1fd2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[['right' 'right' 'down' 'left' 'down']\n",
            " ['right' 'down' 'down' 'right' 'down']\n",
            " ['right' 'right' 'down' 'down' 'down']\n",
            " ['right' 'right' 'right' 'down' 'down']\n",
            " ['up' 'up' 'right' 'right' 'up']]\n",
            "\n",
            "Q-Table:\n",
            "[[ 9.04322966e-01 -7.36814469e-02  1.69017228e-01  2.34328158e+01]\n",
            " [ 3.10095717e+00  9.18346652e+00  1.98846174e+00  4.49977332e+01]\n",
            " [ 1.33039571e+01  5.48168916e+01  9.09018309e+00  9.51583983e+00]\n",
            " [-7.64079734e-01  6.91146074e+00  3.87364411e+01  9.81199402e+00]\n",
            " [-6.70787610e-01  5.94764630e+01 -4.92566003e-01 -6.79346521e-01]\n",
            " [-7.71647482e-01  1.20864282e+00  7.90496297e-01  3.30512746e+01]\n",
            " [ 3.77592436e+00  5.24039986e+01  3.51118408e+00  1.41749611e+01]\n",
            " [ 1.82863420e+01  6.21671248e+01  1.14651381e+01  7.87411088e+00]\n",
            " [ 2.60122885e+00  6.36189844e+00 -4.95276130e-01  6.06326365e+01]\n",
            " [ 7.39296784e+00  7.88551414e+01  3.52941583e+00  2.70200824e+01]\n",
            " [-5.83705682e-01  4.43780125e+00 -5.85198506e-01  3.46813928e+01]\n",
            " [ 1.26830280e+01  2.19381537e+01  3.00407330e+00  6.21041701e+01]\n",
            " [ 2.98617417e+01  7.01899991e+01  3.59522073e+01  4.13919254e+01]\n",
            " [ 1.12488827e+01  7.76024266e+01  1.80555769e+01  1.10293017e+01]\n",
            " [ 6.64924302e+00  8.89861209e+01  1.94559110e+01  3.59496534e+01]\n",
            " [ 4.48248260e+00  1.32428586e+00  3.40709605e+00  5.79269915e+01]\n",
            " [ 8.85427151e+00  2.01273234e+01  8.19644133e+00  7.01856938e+01]\n",
            " [ 4.77373773e+01  4.81811688e+01  4.68090133e+01  7.91000000e+01]\n",
            " [ 4.54354527e+01  8.90000000e+01  6.23965832e+01  8.23388100e+01]\n",
            " [ 2.70487216e+01  9.99998987e+01  4.51495409e+01  3.10631760e+01]\n",
            " [ 3.40048315e+01  1.30399904e+00 -5.85198506e-01  4.72144558e+00]\n",
            " [ 5.97524270e+01  3.57852497e+00 -4.32361323e-01 -3.68290000e-01]\n",
            " [ 7.99152591e+00  1.38513859e+01  7.09712420e+00  8.85739950e+01]\n",
            " [ 7.19238395e+01  7.24203257e+01  6.82914086e+01  1.00000000e+02]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid environment\n",
        "grid_size = (5, 5)  # 5x5 grid\n",
        "num_states = grid_size[0] * grid_size[1]\n",
        "actions = [\"up\", \"down\", \"left\", \"right\"]\n",
        "num_actions = len(actions)\n",
        "\n",
        "# Helper functions to convert between states and coordinates\n",
        "def state_to_coordinates(state):\n",
        "    return divmod(state, grid_size[1])\n",
        "\n",
        "def coordinates_to_state(x, y):\n",
        "    return x * grid_size[1] + y\n",
        "\n",
        "# Initialize rewards and transitions\n",
        "rewards = np.full(num_states, -1)  # -1 reward for every step\n",
        "terminal_state = coordinates_to_state(4, 4)  # Bottom-right corner is terminal\n",
        "rewards[terminal_state] = 100  # High reward for reaching the goal\n",
        "\n",
        "# Function to determine the next state based on the chosen action\n",
        "def take_action(state, action):\n",
        "    x, y = state_to_coordinates(state)\n",
        "\n",
        "    if action == \"up\":\n",
        "        x = max(0, x - 1)\n",
        "    elif action == \"down\":\n",
        "        x = min(grid_size[0] - 1, x + 1)\n",
        "    elif action == \"left\":\n",
        "        y = max(0, y - 1)\n",
        "    elif action == \"right\":\n",
        "        y = min(grid_size[1] - 1, y + 1)\n",
        "\n",
        "    return coordinates_to_state(x, y)\n",
        "\n",
        "# Q-learning parameters\n",
        "q_table = np.zeros((num_states, num_actions))  # Initialize Q-table\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.2  # Exploration probability\n",
        "num_episodes = 500  # Number of episodes\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    state = np.random.randint(0, num_states)  # Start from a random state\n",
        "\n",
        "    while state != terminal_state:\n",
        "        # Choose an action: Exploration vs. Exploitation\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(0, num_actions)  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Exploit best known action\n",
        "\n",
        "        next_state = take_action(state, actions[action])\n",
        "        reward = rewards[next_state]\n",
        "\n",
        "        # Update Q-value using Bellman equation\n",
        "        best_next_action = np.max(q_table[next_state])\n",
        "        q_table[state, action] += alpha * (reward + gamma * best_next_action - q_table[state, action])\n",
        "\n",
        "        state = next_state  # Move to the next state\n",
        "\n",
        "# Analyze agent performance\n",
        "policy = np.argmax(q_table, axis=1)  # Extract best action per state\n",
        "policy_grid = np.array([actions[a] for a in policy]).reshape(grid_size)\n",
        "\n",
        "# Display the results\n",
        "print(\"Optimal Policy:\")\n",
        "print(policy_grid)\n",
        "print(\"\\nQ-Table:\")\n",
        "print(q_table)"
      ]
    }
  ]
}